# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/152QHUf6cyFyn8lnmqNuRiOEbFj0Xl657
"""

#!pip install split-folders
import glob
import os
import cv2
import PIL
from PIL import Image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
  #from keras.layers.normalization import Batch Normalization
from keras.layers import Dense, Conv2D,Conv3D, MaxPooling2D, Flatten, Dropout
from keras import models
from keras import layers
from keras import optimizers
from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
import splitfolders
from multiprocessing import Process
import time
from joblib import parallel_backend, Parallel, delayed
import multiprocessing
import time
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

class Main:
  def __init__(self, png):
    self.png=png
  def resizing(self):
    dataset1=os.listdir(self.png)
    for i in dataset1:
      filepath=os.path.join(self.png, i)
      try:
        image=cv2.imread(filepath)
        output=cv2.resize(image, (128,128))
        cv2.imwrite(os.path.join(filepath),output)
      except:
        print("exception in the filepath", filepath)

  def gray_scale_conversion(self):
    from skimage import io
    folder_path=self.png
    images_path=os.listdir(folder_path)
    for i,image in enumerate(images_path):
      image = io.imread(os.path.join(folder_path+'/'+image), as_gray=True)
    print("Images converted into (128 *128)")

  def count(self):
    count=os.listdir(self.png)
    print(len(count))

preprocessing=Main("/home/student3/png_folder/stage1")
preprocessing.resizing()
preprocessing.gray_scale_conversion()
preprocessing.count()
preprocessing=Main("/home/student3/png_folder/stage1")
preprocessing.resizing()
preprocessing.gray_scale_conversion()
preprocessing.count()
preprocessing=Main("/home/student3/png_folder/stage1")
preprocessing.resizing()
preprocessing.gray_scale_conversion()
preprocessing.count()
print("All preprocessing over")

def create_model():
  model=Sequential()
  input_shape=(128,128,1)
  model.add(Conv2D(16, kernel_size=(3,3), input_shape=input_shape,activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(64, kernel_size=(3,3),activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(128, kernel_size=(3,3),activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(256, kernel_size=(3,3),activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.2))
  model.add(Flatten())
  model.add(Dense(10, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(3, activation='softmax'))
  model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

def training(all_png):
  splitfolders.ratio(all_png, output="split", seed=1, ratio=(.8, .10, .10), group_prefix=None)
  model=create_model()
  train_datagen = ImageDataGenerator(rescale=1./255,zoom_range=0.2,horizontal_flip=True)
  test_datagen = ImageDataGenerator(rescale=1./255)
  valid_datagen = ImageDataGenerator(rescale = 1./255)

  train_generator = train_datagen.flow_from_directory(all_png,target_size=(128, 128),batch_size=32,class_mode='categorical',color_mode="grayscale")
  valid_generator = test_datagen.flow_from_directory(all_png,target_size=(128, 128),batch_size=32,class_mode='categorical',color_mode="grayscale")
  print(train_generator, test_datagen, valid_generator)

  #start_time = time.time()
  history = model.fit_generator(train_generator,  validation_data = valid_generator, epochs=1, verbose=1)
  test_datagen = ImageDataGenerator(rescale = 1./255)
  test_generator = test_datagen.flow_from_directory(all_png,target_size=(128, 128),batch_size=32,class_mode='categorical',color_mode="grayscale")
  loss_and_metrics = model.evaluate(test_generator, batch_size=128)

  print("Accuracy in the test datatset is {}".format(loss_and_metrics))
  print("\n")

all_png = "/home/student3/png_folder/"
t1 = serial(all_png)
print(f'Time taken for serial processing is {t1} sec')

def parallel(all_png):
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Set to an empty string or '-1' to disable GPU

    num_cpu_cores = 1  # Set the number of CPU cores to use

    print("Num of CPU available: ", multiprocessing.cpu_count())
    # Check the number of available GPUs
    num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))
    print("Num GPUs Available: ", num_gpus_available)

    # Use MirroredStrategy for data parallelism if GPUs are available
    if num_gpus_available >= 1:
        strategy = tf.distribute.MirroredStrategy()
        with strategy.scope():
            # Define the CNN model
            start_time = time.time()
            training(all_png)
            end_time = time.time()
            total_time = end_time - start_time
    else:
        print("Running on CPU. No GPUs available.")
        # Define the CNN model (no MirroredStrategy for a single GPU or CPU)
        # model = create_model()
        # Train the model
        #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
        #with strategy.scope():
        print("Running on CPU. No GPUs available.")
        start_time = time.time()
        training(all_png)
        end_time = time.time()
        total_time = end_time - start_time
    return total_time

t2 = parallel(all_png)
print(f'Time taken for parallel processing is {t2} sec')

speed_up = t1 / t2
efficiency = speed_up / multiprocessing.cpu_count()
print(f"\nSpeed Up: {speed_up}")
print(f"Efficiency is: {efficiency}")